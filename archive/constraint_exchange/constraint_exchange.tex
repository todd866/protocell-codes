\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

\title{\textbf{Communication Beyond Information:\\Manifold Expansion via High-Dimensional Coupling}}

\author{Ian Todd\\
Sydney Medical School\\
University of Sydney\\
Sydney, NSW, Australia\\
\texttt{itod2305@uni.sydney.edu.au}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Information-theoretic bounds on communication assume a fixed statistical manifold: messages move probability mass along pre-existing coordinates, and complexity growth is bounded by mutual information. We identify a regime where this assumption fails. When high-dimensional coherent systems couple, they do not exchange discrete tokens---they exchange constraints. This can create new collective coordinates, increasing the effective dimensionality of the joint system beyond the sum of its parts. We formalize this as \textit{manifold expansion}: coupling between high-D systems induces geometric extension of the accessible state space, enabling complexity growth faster than information-theoretic bounds predict. The mechanism is scale-free, appearing in protocellular coordination, collective intelligence, and early structure formation. We provide a geometric characterization, derive conditions for superadditive dimensionality growth, and discuss implications for the origin of coded information systems.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

Information theory provides powerful bounds on communication: channel capacity limits transmission rates, mutual information bounds learning, and data processing inequalities constrain inference \cite{cover2006elements}. These bounds assume a fixed statistical manifold---the space of possible probability distributions over which communication operates.

This assumption is usually invisible. When two agents exchange messages, we take for granted that the receiver's model class is fixed: decoding maps incoming symbols to a pre-existing internal space. Complexity growth is then bounded by how much information the channel can carry.

But what happens when the communicating systems are themselves high-dimensional dynamical objects? When a protocell network coordinates, or when coupled oscillators synchronize, or when interacting agents learn together---the systems don't merely exchange symbols. They \textit{reshape each other's dynamics}.

This paper identifies a regime where standard information bounds fail:

\begin{quote}
\textbf{Core claim}: When high-dimensional coherent systems couple, they exchange constraints rather than tokens. This can create new collective coordinates, increasing effective dimensionality beyond the sum of parts.
\end{quote}

We call this \textit{manifold expansion}: the accessible statistical manifold grows under coupling. This is not a violation of information theory---it is a regime where information theory's foundational assumption (fixed model class) does not hold.

The implications are significant:
\begin{itemize}
    \item Complexity can grow faster than mutual information bounds predict
    \item Communication becomes manifold deformation, not message passing
    \item The mechanism is scale-free: it applies wherever high-D systems interact
\end{itemize}

\subsection{Coherence vs. Information: The Key Distinction}

We distinguish two modes of inter-system interaction:

\begin{definition}[Information transfer]
System $A$ transmits discrete, addressable, copyable tokens to system $B$. The receiver decodes tokens into a pre-existing internal representation. Complexity growth is bounded by channel capacity.
\end{definition}

\begin{definition}[Constraint exchange]
System $A$ couples to system $B$ via shared dynamical modes. The coupling reshapes both systems' accessible state spaces. Complexity growth depends on geometric properties of the coupling, not channel capacity.
\end{definition}

\textbf{Coherence} (high-dimensional phase order) provides the substrate for constraint exchange:
\begin{itemize}
    \item It is the ability of a system to move as a coupled whole
    \item It resists ergodic mixing
    \item It supports nontrivial collective modes
    \item It is \textit{not} information: it is correlation structure, coordination capacity, constraint
\end{itemize}

\textbf{Information} (discrete symbols) emerges \textit{later}:
\begin{itemize}
    \item When coherent dynamics collapse into reproducible invariants
    \item When interaction motifs become stable, transmissible tokens
    \item When the manifold stops expanding and starts compressing
\end{itemize}

The bridge between them:
\begin{quote}
\textit{Codes form when coherence collapses into reproducible invariants. Communication becomes ``information'' when interaction dynamics generate stable, transmissible tokens.}
\end{quote}

%==============================================================================
\section{Background: Information Geometry}
%==============================================================================

Information geometry studies the differential geometry of statistical manifolds \cite{amari2016information,ay2017information,nielsen2020elementary}. A statistical manifold $\mathcal{M}$ is a space of probability distributions, equipped with the Fisher information metric:
\begin{equation}
g_{ij}(\theta) = \mathbb{E}\left[ \frac{\partial \log p(x|\theta)}{\partial \theta_i} \frac{\partial \log p(x|\theta)}{\partial \theta_j} \right]
\end{equation}

Communication and learning are naturally described as motion on this manifold:
\begin{itemize}
    \item Parameter estimation moves along geodesics
    \item KL divergence measures arc length
    \item Natural gradient descent follows the manifold's geometry
\end{itemize}

\textbf{The hidden assumption}: The manifold $\mathcal{M}$ is fixed. The parameterization $\theta$ may change, but the model class---the set of distributions considered---does not.

This assumption is reasonable when:
\begin{itemize}
    \item Systems have fixed, known structure
    \item Communication channels are well-defined
    \item Complexity is measured by localization on a fixed space
\end{itemize}

It fails when:
\begin{itemize}
    \item Interacting systems are high-dimensional and adaptive
    \item Coupling creates new collective modes
    \item The model class itself is shaped by interaction
\end{itemize}

%==============================================================================
\section{Manifold Expansion Under Coupling}
%==============================================================================

\subsection{Setup}

Consider two dynamical systems with state spaces $X_1 \subset \mathbb{R}^{n_1}$ and $X_2 \subset \mathbb{R}^{n_2}$. Each system admits a statistical description on manifolds $\mathcal{M}_1$ and $\mathcal{M}_2$.

\textbf{Standard assumption}: When systems couple, the joint manifold is the product:
\begin{equation}
\mathcal{M}_{12}^{\text{standard}} = \mathcal{M}_1 \times \mathcal{M}_2
\end{equation}
with dimension $\dim(\mathcal{M}_1) + \dim(\mathcal{M}_2)$.

\textbf{Our claim}: For high-dimensional coherent systems, coupling induces a \textit{manifold extension}:
\begin{equation}
\mathcal{M}_{12} \supsetneq \mathcal{M}_1 \times \mathcal{M}_2
\end{equation}

The extended manifold has new tangent directions not present in either system alone.

\subsection{Mechanism: Collective Coordinates}

Let $\phi: X_1 \times X_2 \to X_1 \times X_2$ be the coupled dynamics. Under coupling, new \textit{collective coordinates} can become accessible:

\begin{definition}[Collective coordinate]
A collective coordinate is a function $\psi: X_1 \times X_2 \to \mathbb{R}$ that is:
\begin{enumerate}
    \item Not reducible to functions of $X_1$ or $X_2$ alone
    \item Stable under the coupled dynamics (slow manifold, metastable basin, or conserved quantity)
    \item Statistically identifiable from observations of the coupled system
\end{enumerate}
\end{definition}

Examples:
\begin{itemize}
    \item Phase difference between coupled oscillators \cite{strogatz2000kuramoto,acebron2005kuramoto}
    \item Synchronization manifold coordinates \cite{pikovsky2001synchronization}
    \item Order parameters of collective states (e.g., chimera states) \cite{panaggio2015chimera}
    \item Interface modes at boundaries between systems
\end{itemize}

These coordinates represent genuinely new degrees of freedom: they were not accessible to either system in isolation.

\subsection{Dimensionality Growth}

We emphasize that ``dimension'' here is not ambient state-space dimension. We study the dimension of the \textit{identifiable statistical family} induced by (i) the coupled dynamics and (ii) a finite observation map or sufficient-statistics set. Coupling can create new \textit{collective observables} that are stable and identifiable, thereby adding new significant Fisher directions even when the underlying product state space is unchanged.

\begin{definition}[Effective dimensionality]
Let $h: X \to \mathbb{R}^m$ be an observation map, and let $\mathcal{M}_h$ be the family of distributions over observations induced by dynamics on $X$. The effective dimensionality $d_{\text{eff}}(\mathcal{M}_h)$ is the participation ratio of the Fisher information spectrum \cite{gao2017theory,stringer2019high}:
\begin{equation}
d_{\text{eff}} = \frac{(\sum_i \lambda_i)^2}{\sum_i \lambda_i^2}
\end{equation}
where $\lambda_i$ are eigenvalues of the Fisher information matrix on $\mathcal{M}_h$.
\end{definition}

\begin{proposition}[Superadditive dimensionality]
\label{prop:superadditive}
For high-dimensional systems with coherent coupling, there exist regimes where the effective dimensionality of the joint system exceeds the sum of parts:
\begin{equation}
d_{\text{eff}}(\mathcal{M}_{12,h}) > d_{\text{eff}}(\mathcal{M}_{1,h_1}) + d_{\text{eff}}(\mathcal{M}_{2,h_2})
\end{equation}
This occurs when coupling creates new collective observables that become statistically identifiable---new significant directions in Fisher space.
\end{proposition}

\textit{Sketch of argument}: Coupling between high-D systems can:
\begin{enumerate}
    \item Stabilize modes that were unstable in isolation (new attractors)
    \item Create synchronization manifolds with their own geometry
    \item Generate interface dynamics at boundaries
    \item Unlock latent degrees of freedom via resonance
\end{enumerate}

Each mechanism adds new statistically identifiable coordinates to the joint system.

\textbf{Toy example}: Consider two populations of coupled oscillators. In isolation, each population has a distribution over phases; the identifiable statistics are the mean phase and phase dispersion of each population. Under weak coupling, a new observable becomes identifiable: the \textit{phase difference} between populations. This collective coordinate was not measurable before coupling (no reference) and is now a stable, reproducible feature of the joint dynamics. The Fisher information matrix gains a new significant eigenvalue corresponding to this collective mode. The rank/participation ratio increases---superadditive dimensionality.

\begin{remark}
This is \textit{not} a violation of any conservation law. The ambient product state space $X_1 \times X_2$ hasn't changed. What's changed is the \textit{identifiable statistical family}---the set of distributions that are stable, accessible, and distinguishable under observation.
\end{remark}

\subsection{When Expansion Dominates}

Manifold expansion is most pronounced when:
\begin{enumerate}
    \item \textbf{High internal dimensionality}: Each system has many coupled degrees of freedom
    \item \textbf{Coherent dynamics}: Internal correlations are strong (non-ergodic, structured)
    \item \textbf{Weak-to-intermediate coupling}: Strong enough to create collective modes, weak enough to avoid synchronization collapse
    \item \textbf{Heterogeneity}: Systems are different enough that coupling is non-trivial
\end{enumerate}

In the opposite regime (low-D, incoherent, very weak or very strong coupling), the standard product assumption holds.

%==============================================================================
\section{Consequences for Complexity Growth}
%==============================================================================

\subsection{Faster Than Information Bounds Predict}

Standard bounds on complexity growth (learning rate, adaptation speed, structure formation rate) assume:
\begin{itemize}
    \item Fixed parameterization
    \item Fixed sufficient statistics
    \item Fixed dimensionality
\end{itemize}

When coupling creates new coordinates:
\begin{itemize}
    \item The Fisher geometry changes because the model class changes---new significant Fisher directions appear
    \item KL-based learning rates underpredict adaptation
    \item Channel capacity arguments don't apply (the ``channel'' itself is being restructured)
\end{itemize}

\begin{proposition}[Complexity acceleration]
In the manifold expansion regime, complexity growth rate $\dot{C}$ can exceed information-theoretic bounds:
\begin{equation}
\dot{C}_{\text{coupled}} > \dot{C}_{\text{info-bound}}
\end{equation}
where $\dot{C}_{\text{info-bound}}$ is derived from mutual information between systems.
\end{proposition}

The ``extra'' complexity comes from geometric expansion, not information transfer.

\subsection{Colony Dynamics}

Consider $N$ high-D systems in a ``colony'' (weakly coupled network). If each pairwise coupling can create $k$ new collective coordinates, the potential dimensionality growth is superlinear in $N$:
\begin{equation}
d_{\text{eff}}(\mathcal{M}_{\text{colony}}) \sim N \cdot d_{\text{eff}}(\mathcal{M}_{\text{single}}) + O(|E|) \cdot k
\end{equation}
where $|E|$ is the number of coupling edges (dense networks: $|E| \sim N^2$; sparse networks: $|E| \sim N$). Redundancy and synchronization collapse can saturate this growth.

This explains why:
\begin{itemize}
    \item Interacting protocell networks can develop complexity faster than isolated cells
    \item Collective intelligence exceeds sum of individual intelligences
    \item Early structure formation can appear ``too fast'' for hierarchical assembly models
\end{itemize}

\subsection{Thermodynamic Cost of Expansion}

Superlinear complexity is an \textit{interaction dividend}, but it has an \textit{overhead cost} paid continuously as entropy production:
\begin{equation}
\text{net complexity growth} \;\sim\; \underbrace{f(N, \text{coupling})}_{\text{superlinear}} \;-\; \underbrace{g(\text{maintenance}, \text{sync}, \text{repair})}_{\propto \dot{S}_{\text{prod}}}
\end{equation}

Keeping many channels open, synchronized, and robust against noise requires continuous dissipation: maintaining gradients, pumping ions, error correction, policing free-riders \cite{england2013statistical,horowitz2020thermodynamic}. The interaction overhead scales with the number of active collective modes.

This explains major transitions in complexity history \cite{szathmary1995major}. The Cambrian explosion, for instance, was primarily a \textit{metabolic revolution}: increased oxygen and energetic throughput raised the sustainable entropy production rate, making high-overhead coordination architectures (nervous systems, active movement, developmental patterning) suddenly \textit{affordable} \cite{sperling2013oxygen,erwin2011cambrian,knoll2014paleobiological}.

\textbf{Falsifiable prediction}: Step-changes in available free-energy flux should correlate with:
\begin{enumerate}
    \item Increased interaction density (more coupling channels)
    \item Increased specialization (division of labor requires coordination)
    \item Increased control-layer complexity (more regulatory machinery)
\end{enumerate}
more strongly than they correlate with raw genome size or species count.

Complex systems didn't just ``appear.'' They became \textit{affordable}.

The same logic applies to artificial intelligence. In AI, ``metabolism'' is continuous compute throughput: GPUs, electricity, cooling, infrastructure---the entropy production budget that keeps the system far from equilibrium. The ``overhead'' is training (dissipative search), inference at scale (persistent compute burn), coordination layers (tools, memory, retrieval), and alignment/monitoring (error correction).

The current AI revolution is fundamentally a \textit{metabolic explosion}: a step-change in sustainable compute flux that makes high-overhead coordination architectures suddenly affordable \cite{kaplan2020scaling,hoffmann2022training}. Once the power budget crosses threshold, you can afford dense model-to-model interaction, faster iteration cycles, and richer ecologies of specialized services.

\begin{quote}
\textit{AI is the moment when superlinear interaction in information-space becomes cheap enough to explode.}
\end{quote}

The Cambrian was powered by a step-change in usable energy flux; the AI transition is powered by a step-change in usable \textit{compute} flux. The parallel is structural, not metaphorical.

\subsection{The Compression Transition}

Manifold expansion cannot continue indefinitely. Eventually:
\begin{itemize}
    \item Collective modes stabilize into reproducible invariants
    \item Interaction motifs become discrete, re-identifiable tokens
    \item The system transitions from ``coherence mode'' to ``information mode''
\end{itemize}

This is the origin of codes:
\begin{quote}
\textit{Codes form when the manifold stops expanding and starts compressing.}
\end{quote}

In biological terms: genetic codes (DNA/RNA/protein) are not the origin of meaning---they are the \textit{compression and stabilization} of meaning that already existed in interaction dynamics \cite{chen2004emergence,adamski2020protocells}.

%==============================================================================
\section{Application: Origin of Coded Systems}
%==============================================================================

\subsection{The Sequencing Claim}

We propose a general ordering:
\begin{equation}
\text{External communication} \to \text{Internal communication}
\end{equation}

Before a system can ``talk to itself'' (stable internal memory + translation), it must coordinate with other systems in a shared environment. The earliest selective pressures are external: don't get dissolved, outcompeted, or decoupled.

This predicts a developmental pathway:
\begin{enumerate}
    \item \textbf{Coupling mechanisms}: Phase-locking, chemical cross-catalysis, membrane exchange, contact signaling
    \item \textbf{Shared conventions}: Repeatable interaction motifs that function as proto-signals
    \item \textbf{Tokenization}: Some motifs become discrete, re-identifiable objects
    \item \textbf{Internal memory}: The token set becomes storable/replicable $\to$ genetic stack
\end{enumerate}

\subsection{DNA-RNA-Protein as Communication System}

The genetic machinery is literally a layered encoding/transmission/decoding stack:
\begin{center}
\begin{tabular}{ll}
\toprule
Layer & Function \\
\midrule
DNA & Persistent, high-fidelity storage (slow, stable channel) \\
RNA & Transmittable, transient message (routing + control) \\
Protein & Embodied actuation / phenotype (receiver output) \\
\bottomrule
\end{tabular}
\end{center}

This is not metaphor---it is communication in the technical sense. But it is \textit{late} communication: the compression of coordination dynamics that existed earlier.

\subsection{The Killer Distinction}

\textbf{High-D coherence is not information.}

This prevents the common category error of treating ``structured dynamics'' as ``encoded symbols.''
\begin{itemize}
    \item Coherence = constraint, correlation, coordination capacity
    \item Information = discrete, addressable, copyable, decodable distinctions
\end{itemize}

The bridge: codes form when coherence collapses into reproducible invariants.

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Scale-Free Mechanism}

The manifold expansion mechanism is geometric, not substrate-specific. It should appear wherever:
\begin{itemize}
    \item High-dimensional coherent systems exist
    \item Coupling can create new collective modes
    \item Selective pressure favors stable coordination
\end{itemize}

This includes:
\begin{itemize}
    \item Protocellular networks (origin of life)
    \item Neural populations (collective computation)
    \item Social systems (collective intelligence)
    \item Early cosmic structure (if ``universes'' or ``domains'' interact)
\end{itemize}

\subsection{Why Non-Stochastic Models Are Expensive}

A deterministic model of $N$ interacting high-D systems must carry:
\begin{itemize}
    \item State of every system
    \item Every boundary mode
    \item Every coupling channel
    \item Enough history to capture path-dependence
\end{itemize}

This is exponentially expensive. Stochastic models with strong constraints (selection, conservation, interface costs) can capture effective behavior without full specification.

\textbf{Stochasticity is not a cop-out}: it is the acknowledgment that macroscopic order comes from constraints shaping exploration, not from a fully scripted trajectory.

\subsection{Testable Predictions}

\begin{enumerate}
    \item \textbf{Superlinear complexity scaling}: Coupled high-D systems should show complexity growth superlinear in group size
    \item \textbf{Phase transitions}: Sudden jumps in effective dimensionality when coupling crosses thresholds
    \item \textbf{History dependence}: Path-dependent ``protocol'' formation in interacting colonies
    \item \textbf{Compression after expansion}: Tokenization/code formation should follow (not precede) stable collective mode formation
\end{enumerate}

%==============================================================================
\section{Conclusion}
%==============================================================================

Information geometry has been remarkably successful because the fixed-manifold assumption usually holds. We have identified a regime where it does not: when high-dimensional coherent systems couple, they exchange constraints rather than tokens, and the statistical manifold itself expands.

This is not a rejection of information theory. It is a geometric extension: communication as manifold deformation, not just message passing.

The mechanism explains why complexity can grow faster than information bounds predict, why colonies outperform individuals, and why codes appear after---not before---coordination dynamics stabilize.

\begin{quote}
\textit{High-D systems don't primarily communicate by transmitting information; they communicate by deforming each other's dynamics.}
\end{quote}

The formalization we provide---superadditive dimensionality, collective coordinates, compression transition---gives this intuition geometric content. It is testable, falsifiable, and connects phenomena across scales: from protocells to neural populations to cosmic structure.

\vspace{2em}

\noindent\textbf{Code availability}: Supplementary code is available at \url{https://github.com/todd866/manifold-expansion}

\noindent\textbf{Companion paper}: For application of manifold expansion to the origin of coded information in protocellular networks, see: I.~Todd, ``Codes as Coordination: How Coupled Protocells Generate Symbolic Communication Without Genes'' (submitted to \textit{BioSystems}).

\vspace{2em}
\hrule

\bibliographystyle{plain}
\begin{thebibliography}{99}

% Information Geometry foundations
\bibitem{amari2016information}
S.-I. Amari.
\textit{Information Geometry and Its Applications}.
Springer, 2016.

\bibitem{ay2017information}
N.~Ay, J.~Jost, H.~V. Lê, and L.~Schwachhöfer.
\textit{Information Geometry}.
Springer, 2017.

\bibitem{nielsen2020elementary}
F.~Nielsen.
An elementary introduction to information geometry.
\textit{Entropy}, 22(10):1100, 2020.

% Information theory
\bibitem{cover2006elements}
T.~M. Cover and J.~A. Thomas.
\textit{Elements of Information Theory}.
Wiley, 2nd edition, 2006.

% Effective dimensionality / participation ratio
\bibitem{gao2017theory}
P.~Gao, E.~Trautmann, B.~Yu, G.~Santhanam, S.~Ryu, K.~Shenoy, and S.~Ganguli.
A theory of multineuronal dimensionality, dynamics and measurement.
\textit{bioRxiv}, 214262, 2017.

\bibitem{litwin2017optimal}
E.~Litwin-Kumar, K.~D. Harris, R.~Axel, H.~Sompolinsky, and L.~F. Abbott.
Optimal degrees of synaptic connectivity.
\textit{Neuron}, 93(5):1153--1164, 2017.

\bibitem{stringer2019high}
C.~Stringer, M.~Pachitariu, N.~Steinmetz, C.~B. Reddy, M.~Carandini, and K.~D. Harris.
High-dimensional geometry of population responses in visual cortex.
\textit{Nature}, 571(7765):361--365, 2019.

% Collective dynamics and synchronization
\bibitem{strogatz2000kuramoto}
S.~H. Strogatz.
From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators.
\textit{Physica D}, 143(1--4):1--20, 2000.

\bibitem{pikovsky2001synchronization}
A.~Pikovsky, M.~Rosenblum, and J.~Kurths.
\textit{Synchronization: A Universal Concept in Nonlinear Sciences}.
Cambridge University Press, 2001.

\bibitem{acebron2005kuramoto}
J.~A. Acebrón, L.~L. Bonilla, C.~J. Pérez~Vicente, F.~Ritort, and R.~Spigler.
The Kuramoto model: A simple paradigm for synchronization phenomena.
\textit{Reviews of Modern Physics}, 77(1):137--185, 2005.

% Chimera states and collective modes
\bibitem{panaggio2015chimera}
M.~J. Panaggio and D.~M. Abrams.
Chimera states: coexistence of coherence and incoherence in networks of coupled oscillators.
\textit{Nonlinearity}, 28(3):R67--R87, 2015.

% Metabolic transitions in evolution
\bibitem{knoll2014paleobiological}
A.~H. Knoll and M.~A. Nowak.
The timetable of evolution.
\textit{Science Advances}, 3(5):e1603076, 2017.

\bibitem{erwin2011cambrian}
D.~H. Erwin, M.~Laflamme, S.~M. Tweedt, E.~A. Sperling, D.~Pisani, and K.~J. Peterson.
The Cambrian conundrum: early divergence and later ecological success in the early history of animals.
\textit{Science}, 334(6059):1091--1097, 2011.

\bibitem{sperling2013oxygen}
E.~A. Sperling, C.~A. Frieder, A.~V. Raman, P.~R. Girguis, L.~A. Levin, and A.~H. Knoll.
Oxygen, ecology, and the Cambrian radiation of animals.
\textit{Proceedings of the National Academy of Sciences}, 110(33):13446--13451, 2013.

% Thermodynamics of complexity
\bibitem{england2013statistical}
J.~L. England.
Statistical physics of self-replication.
\textit{Journal of Chemical Physics}, 139(12):121923, 2013.

\bibitem{horowitz2020thermodynamic}
J.~M. Horowitz and T.~R. Gingrich.
Thermodynamic uncertainty relations constrain non-equilibrium fluctuations.
\textit{Nature Physics}, 16(1):15--20, 2020.

% AI scaling and compute
\bibitem{hoffmann2022training}
J.~Hoffmann et al.
Training compute-optimal large language models.
\textit{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{kaplan2020scaling}
J.~Kaplan et al.
Scaling laws for neural language models.
\textit{arXiv preprint arXiv:2001.08361}, 2020.

% Origin of life and coordination
\bibitem{chen2004emergence}
I.~A. Chen and P.~Walde.
From self-assembled vesicles to protocells.
\textit{Cold Spring Harbor Perspectives in Biology}, 2(7):a002170, 2010.

\bibitem{adamski2020protocells}
P.~Adamski, M.~Eleveld, A.~Sood, A.~Kun, A.~Szilágyi, T.~Czárán, E.~Szathmáry, and S.~Otto.
From self-replication to replicator systems \textit{en route} to de novo life.
\textit{Nature Reviews Chemistry}, 4(8):386--403, 2020.

% Major transitions
\bibitem{szathmary1995major}
J.~Maynard~Smith and E.~Szathmáry.
\textit{The Major Transitions in Evolution}.
Oxford University Press, 1995.

\end{thebibliography}

\end{document}
